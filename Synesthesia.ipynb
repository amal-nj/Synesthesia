{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Synesthesia.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaS9A02YOs/WA0O60NFK5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amal-nj/Synesthesia/blob/main/Synesthesia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuqpFy8w_6LL"
      },
      "source": [
        "# Synesthesia\n",
        "###AI Artathon\n",
        "This notebook is composed of 3 different models and all the processing in between to produce the final output:\n",
        "- Timbre transfer: takes an audio file, a choice of instrument and produces a new version of the input audio played with the chosen instrument\n",
        "-\tDeep visualizer: a music visualizer that takes the output file of the timbre transfer model and produces music visualization based on pitch, a number of chose class and a few other parameters. \n",
        "-\tStyle transfer: To further process the produced video. Style transfer is applied on every frame in the video separately. Eventually, all frames are compiled together with the audio from the first model.Synesthesia\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptcZTmqUP5IH"
      },
      "source": [
        "#Timber transfer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDt4jZwyoRKp"
      },
      "source": [
        "#@title #Install and Import\n",
        "\n",
        "#@markdown Install ddsp, define some helper functions, and download the model. This transfers a lot of data and _should take a minute or two_.\n",
        "print('Installing from pip package...')\n",
        "!pip install -qU ddsp==1.6.5\n",
        "\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "import crepe\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.colab.colab_utils import (\n",
        "    auto_tune, get_tuning_factor, download, \n",
        "    play, record, specplot, upload, \n",
        "    DEFAULT_SAMPLE_RATE)\n",
        "from ddsp.training.postprocessing import (\n",
        "    detect_notes, fit_quantile_transform\n",
        ")\n",
        "import gin\n",
        "from google.colab import files\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
        "\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aim9HXzVoRKr"
      },
      "source": [
        "#@title Record or Upload Audio\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
        "\n",
        "record_or_upload = \"Upload (.mp3 or .wav)\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "record_seconds =     5#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  audio = record(seconds=record_seconds)\n",
        "else:\n",
        "  # Load audio sample here (.mp3 or .wav3 file)\n",
        "  # Just use the first file.\n",
        "  filenames, audios = upload()\n",
        "  audio = audios[0]\n",
        "if len(audio.shape) == 1:\n",
        "  audio = audio[np.newaxis, :]\n",
        "print('\\nExtracting audio features...')\n",
        "\n",
        "# Plot.\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n",
        "# Setup the session.\n",
        "ddsp.spectral_ops.reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "start_time = time.time()\n",
        "audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
        "audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "audio_features_mod = None\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "\n",
        "TRIM = -15\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=3, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 8))\n",
        "ax[0].plot(audio_features['loudness_db'][:TRIM])\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
        "ax[1].set_ylabel('f0 [midi]')\n",
        "\n",
        "ax[2].plot(audio_features['f0_confidence'][:TRIM])\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "_ = ax[2].set_xlabel('Time step [frame]')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "NYBY7VsQoRKs"
      },
      "source": [
        "#@title Load a model\n",
        "#@markdown Run for ever new audio input\n",
        "model = 'Flute' #@param ['Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone', 'Upload your own (checkpoint folder as .zip)']\n",
        "MODEL = model\n",
        "\n",
        "\n",
        "def find_model_dir(dir_name):\n",
        "  # Iterate through directories until model directory is found\n",
        "  for root, dirs, filenames in os.walk(dir_name):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n",
        "        model_dir = root\n",
        "        break\n",
        "  return model_dir \n",
        "\n",
        "if model in ('Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone'):\n",
        "  # Pretrained models.\n",
        "  PRETRAINED_DIR = '/content/pretrained'\n",
        "  # Copy over from gs:// for faster loading.\n",
        "  !rm -r $PRETRAINED_DIR &> /dev/null\n",
        "  !mkdir $PRETRAINED_DIR &> /dev/null\n",
        "  GCS_CKPT_DIR = 'gs://ddsp/models/timbre_transfer_colab/2021-07-08'\n",
        "  model_dir = os.path.join(GCS_CKPT_DIR, 'solo_%s_ckpt' % model.lower())\n",
        "  \n",
        "  !gsutil cp $model_dir/* $PRETRAINED_DIR &> /dev/null\n",
        "  model_dir = PRETRAINED_DIR\n",
        "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "else:\n",
        "  # User models.\n",
        "  UPLOAD_DIR = '/content/uploaded'\n",
        "  !mkdir $UPLOAD_DIR\n",
        "  uploaded_files = files.upload()\n",
        "\n",
        "  for fnames in uploaded_files.keys():\n",
        "    print(\"Unzipping... {}\".format(fnames))\n",
        "    !unzip -o \"/content/$fnames\" -d $UPLOAD_DIR &> /dev/null\n",
        "  model_dir = find_model_dir(UPLOAD_DIR)\n",
        "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "\n",
        "# Load the dataset statistics.\n",
        "DATASET_STATS = None\n",
        "dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
        "print(f'Loading dataset statistics from {dataset_stats_file}')\n",
        "try:\n",
        "  if tf.io.gfile.exists(dataset_stats_file):\n",
        "    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
        "      DATASET_STATS = pickle.load(f)\n",
        "except Exception as err:\n",
        "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
        "\n",
        "\n",
        "# Parse gin config,\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
        "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "ckpt_name = ckpt_files[0].split('.')[0]\n",
        "ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
        "n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "time_steps = int(audio.shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "# print(\"===Trained model===\")\n",
        "# print(\"Time Steps\", time_steps_train)\n",
        "# print(\"Samples\", n_samples_train)\n",
        "# print(\"Hop Size\", hop_size)\n",
        "# print(\"\\n===Resynthesis===\")\n",
        "# print(\"Time Steps\", time_steps)\n",
        "# print(\"Samples\", n_samples)\n",
        "# print('')\n",
        "\n",
        "gin_params = [\n",
        "    'Harmonic.n_samples = {}'.format(n_samples),\n",
        "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "    'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
        "    'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
        "]\n",
        "\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config(gin_params)\n",
        "\n",
        "\n",
        "# Trim all input vectors to correct lengths \n",
        "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "  audio_features[key] = audio_features[key][:time_steps]\n",
        "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "model.restore(ckpt)\n",
        "\n",
        "# Build model by running a batch through it.\n",
        "start_time = time.time()\n",
        "_ = model(audio_features, training=False)\n",
        "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pGJQEmPCoRKt"
      },
      "source": [
        "#@title Modify conditioning\n",
        "\n",
        "#@markdown These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). \n",
        "\n",
        "\n",
        "#@markdown ## Note Detection\n",
        "\n",
        "#@markdown You can leave this at 1.0 for most cases\n",
        "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
        "\n",
        "\n",
        "#@markdown ## Automatic\n",
        "\n",
        "ADJUST = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Quiet parts without notes detected (dB)\n",
        "quiet = 20 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
        "\n",
        "#@markdown Force pitch to nearest note (amount)\n",
        "autotune = 0 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ## Manual\n",
        "\n",
        "\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  -1 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "\n",
        "#@markdown Adjust the overall loudness (dB)\n",
        "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shift=0.0):\n",
        "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "  audio_features['loudness_db'] += ld_shift\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, pitch_shift=0.0):\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
        "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "mask_on = None\n",
        "\n",
        "if ADJUST and DATASET_STATS is not None:\n",
        "  # Detect sections that are \"on\".\n",
        "  mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n",
        "                                        audio_features['f0_confidence'],\n",
        "                                        threshold)\n",
        "\n",
        "  if np.any(mask_on):\n",
        "    # Shift the pitch register.\n",
        "    target_mean_pitch = DATASET_STATS['mean_pitch']\n",
        "    pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
        "    mean_pitch = np.mean(pitch[mask_on])\n",
        "    p_diff = target_mean_pitch - mean_pitch\n",
        "    p_diff_octave = p_diff / 12.0\n",
        "    round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
        "    p_diff_octave = round_fn(p_diff_octave)\n",
        "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
        "\n",
        "\n",
        "    # Quantile shift the note_on parts.\n",
        "    _, loudness_norm = fit_quantile_transform(\n",
        "        audio_features['loudness_db'],\n",
        "        mask_on,\n",
        "        inv_quantile=DATASET_STATS['quantile_transform'])\n",
        "\n",
        "    # Turn down the note_off parts.\n",
        "    mask_off = np.logical_not(mask_on)\n",
        "    loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
        "    loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
        "    \n",
        "    audio_features_mod['loudness_db'] = loudness_norm \n",
        "\n",
        "    # Auto-tune.\n",
        "    if autotune:\n",
        "      f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "      tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
        "      f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n",
        "      audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
        "\n",
        "  else:\n",
        "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
        "\n",
        "else:\n",
        "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
        "\n",
        "# Manual Shifts.\n",
        "audio_features_mod = shift_ld(audio_features_mod, loudness_shift)\n",
        "audio_features_mod = shift_f0(audio_features_mod, pitch_shift)\n",
        "\n",
        "\n",
        "\n",
        "# Plot Features.\n",
        "has_mask = int(mask_on is not None)\n",
        "n_plots = 3 if has_mask else 2 \n",
        "fig, axes = plt.subplots(nrows=n_plots, \n",
        "                      ncols=1, \n",
        "                      sharex=True,\n",
        "                      figsize=(2*n_plots, 8))\n",
        "\n",
        "if has_mask:\n",
        "  ax = axes[0]\n",
        "  ax.plot(np.ones_like(mask_on[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(note_on_value[:TRIM])\n",
        "  ax.plot(mask_on[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask')\n",
        "  ax.set_xlabel('Time step [frame]')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "ax = axes[0 + has_mask]\n",
        "ax.plot(audio_features['loudness_db'][:TRIM])\n",
        "ax.plot(audio_features_mod['loudness_db'][:TRIM])\n",
        "ax.set_ylabel('loudness_db')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axes[1 + has_mask]\n",
        "ax.plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(audio_features_mod['f0_hz'][:TRIM]))\n",
        "ax.set_ylabel('f0 [midi]')\n",
        "_ = ax.legend(['Original','Adjusted'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBu-tOecoRKx"
      },
      "source": [
        "#@title #Resynthesize Audio\n",
        "\n",
        "af = audio_features if audio_features_mod is None else audio_features_mod\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "outputs = model(af, training=False)\n",
        "audio_gen = model.get_audio_from_outputs(outputs)\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen)\n",
        "\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "specplot(audio_gen)\n",
        "_ = plt.title(\"Resynthesis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otXnHLKtPcH2"
      },
      "source": [
        "The audio generated by the model is saved in mp3 format for further processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0oezNmjo96S"
      },
      "source": [
        "# Deep Visualizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfjNAllH6Rp7"
      },
      "source": [
        "!git clone https://github.com/Ease78/deep-music-visualizer\n",
        "\n",
        "%cd deep-music-visualizer\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHShGHHf6Sgt"
      },
      "source": [
        "!python visualize.py --song flute-raise-me-up-first.mp3 --resolution 256 --duration 3 --pitch_sensitivity 280 --tempo_sensitivity 0.8 --depth 0.3 --classes 985 979 978 973 966 960 889 614 594 568 536 420 --output_file flute-raise-me-up-first.mp4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uIC2Ybv6qGh"
      },
      "source": [
        "# Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ajP_u73s6m"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxUicSPUOP6"
      },
      "source": [
        "### Import and configure modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyftRTSMuwue"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "# Load compressed models from tensorflow_hub\n",
        "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1OLbOWhPCO"
      },
      "source": [
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM6VEGrGLh62"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor*255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor)>3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "\n",
        "  return PIL.Image.fromarray(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLljcwv5qZs"
      },
      "source": [
        "def load_img_origin(path_to_img):\n",
        "  max_dim = 512\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Cj7A3EY3fX"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aogUg9rBbPba"
      },
      "source": [
        "style_image = load_img_origin('content/cancer_cell_5.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtqtkasHPqzw"
      },
      "source": [
        "remove the folders if they already contain frames from previouse runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyW9DS3u1Hi8"
      },
      "source": [
        "!rm -r results\n",
        "!rm -r preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmEURcU3bmKZ"
      },
      "source": [
        "!mkdir preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4dLxciKbrUf"
      },
      "source": [
        "!mkdir results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekW51QANP4JA"
      },
      "source": [
        "We now loop through every frame in the video generated from the previous step, apply style transfer to it and save it in a separate folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5r4O8A_mI22"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "cap = cv2.VideoCapture(\"/content/deep-music-visualizer/flute-raise-me-up-first.mp4\")\n",
        "while not cap.isOpened():\n",
        "    cap = cv2.VideoCapture(\"/content/deep-music-visualizer/flute-raise-me-up-first.mp4\")\n",
        "    cv2.waitKey(1000)\n",
        "    print(\"Wait for the header\")\n",
        "\n",
        "pos_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "count = 0\n",
        "random_var = True\n",
        "while True:\n",
        "    flag, frame = cap.read()\n",
        "    if flag:\n",
        "        # The frame is ready and already captured\n",
        "        # cv2_imshow(frame)\n",
        "        \n",
        "        count +=1\n",
        "        cv2.imwrite(\"/content/preprocess/frame%d.jpg\" % count, frame)\n",
        "        path = f\"/content/preprocess/frame{count}.jpg\"\n",
        "\n",
        "        content = load_img_origin(path)\n",
        "        stylized_image = hub_model(tf.constant(content), tf.constant(style_image))[0]\n",
        "        img = tensor_to_image(stylized_image)\n",
        "\n",
        "        cv2.imwrite(f\"/content/results/frame{count}.jpg\", np.asarray(img))\n",
        "        # display(img)\n",
        "        pos_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "        print(str(pos_frame)+\" frames\")\n",
        "    else:\n",
        "        # The next frame is not ready, so we try to read it again\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, pos_frame-1)\n",
        "        print(\"frame is not ready\")\n",
        "        # It is better to wait for a while for the next frame to be ready\n",
        "        cv2.waitKey(1000)\n",
        "\n",
        "    if cv2.waitKey(10) == 27:\n",
        "        break\n",
        "    if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
        "        # If the number of captured frames is equal to the total number of frames,\n",
        "        # we stop\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZkUP3EPQau0"
      },
      "source": [
        "after all frames are generated we combine them in one video again while maintaining the same frame rate per second of the original video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-LSX2kG2T1W"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#@title Generate video using ffmpeg\n",
        "\n",
        "init_frame =  1#@param  {type: \"number\"}\n",
        "last_frame =  601#@param {type: \"number\"}\n",
        "duration =  15#@param {type: \"number\"}\n",
        "\n",
        "# min_fps =  30#@param {type: \"number\"}\n",
        "# max_fps =  30#@param {type: \"number\"}\n",
        "\n",
        "total_frames = last_frame-init_frame\n",
        "\n",
        "min_fps =  (total_frames/duration)\n",
        "max_fps =  min_fps\n",
        "# Desired video runtime in seconds\n",
        "length =  1#@param {type: \"number\"}\n",
        "use_upscaled_images = False #@param {type: \"boolean\"}\n",
        "frames = []\n",
        "tqdm.write('Generating video...')\n",
        "\n",
        "\n",
        "for i in range(init_frame,last_frame): #\n",
        "  filename = f\"/content/results/frame{i}.jpg\"\n",
        "  frames.append(Image.open(filename))\n",
        "\n",
        "#fps = last_frame/10\n",
        "fps = np.clip(total_frames/length,min_fps,max_fps)\n",
        "\n",
        "# Names the video after the prompt if there is one, if not, defaults to video.mp4\n",
        "def listToString(s): \n",
        "    \n",
        "    # initialize an empty string\n",
        "    str1 = \"\" \n",
        "    \n",
        "    # traverse in the string  \n",
        "    for ele in s: \n",
        "        str1 += ele  \n",
        "    \n",
        "    # return string  \n",
        "    return str1 \n",
        "        \n",
        "\n",
        "video_filename = \"flute-raise-me-up-first-2\" #@param {type: \"string\"}\n",
        "#@markdown Note: using images previously upscaled by ESRGAN may take longer to generate\n",
        "\n",
        "\n",
        "video_filename = listToString(video_filename).replace(\" \",\"_\")\n",
        "print(\"Video filename: \"+ video_filename)\n",
        "\n",
        "video_filename = video_filename + \".mp4\"\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', video_filename], stdin=PIPE)\n",
        "for im in tqdm(frames):\n",
        "    im.save(p.stdin, 'PNG')\n",
        "p.stdin.close()\n",
        "\n",
        "print(\"Compressing video...\")\n",
        "p.wait()\n",
        "\n",
        "print(\"Video ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXbKwZo_Q9Xz"
      },
      "source": [
        "Finally, recombine the audio with the stylized video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkK4oDnNdyg8"
      },
      "source": [
        "def combine_audio(vidname, audname, outname, fps=25):\n",
        "    import moviepy.editor as mpe\n",
        "    my_clip = mpe.VideoFileClip(vidname)\n",
        "    audio_background = mpe.AudioFileClip(audname)\n",
        "    final_clip = my_clip.set_audio(audio_background)\n",
        "    final_clip.write_videofile(outname,fps=fps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5axBqKudzVL"
      },
      "source": [
        "combine_audio(video_filename, \"/content/flute-raise-me-up-first.mp3\", \"/flute-raise-me-up-first-audio.mp4\", fps=min_fps)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}